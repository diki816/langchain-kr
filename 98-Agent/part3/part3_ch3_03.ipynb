{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict, List, Dict, Sequence\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    outline: Dict[str, str]\n",
    "    current_section: int\n",
    "    section_content: str\n",
    "    section_image: str\n",
    "    image_prompt: str\n",
    "    total_sections: int\n",
    "    full_report: List[Dict[str, str]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search = TavilySearchResults(max_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개요작성 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field, create_model\n",
    "\n",
    "# 사용자로부터 입력받은 전체 섹션 수에 따라 Data model의 key와 value를 정의하기 위한 함수\n",
    "# create_model 함수는 Pydantic Data model의 이름과 key:value 값을 기반으로 동적인 Data model을 만둘 수 있게 보조\n",
    "def create_outline_model(section_count: int):\n",
    "    fields = {f\"section{i}\":(str, Field(description=f\"Title for section {i}\")) for i in range(1, section_count + 1)}\n",
    "    return create_model(\"DynamicOutline\", **fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outline_generator(state: State):\n",
    "    DynamicOutline = create_outline_model(state[\"total_sections\"])\n",
    "    outline_parser = JsonOutputParser(pydantic_object=DynamicOutline)\n",
    "\n",
    "    outline_prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Crate an outlne for a detailed report with exactly {section_count} main sections.\n",
    "        {format_instructions}\n",
    "        The topic is: {topic}\n",
    "        \"\"\",\n",
    "        input_variables=[\"section_count\", \"topic\"],\n",
    "        partial_variables={\"format_instructions\": outline_parser.get_format_instructions()},\n",
    "    )\n",
    "    chain = outline_prompt | llm | outline_parser\n",
    "\n",
    "    outline = chain.invoke({\n",
    "        \"section_count\": state[\"total_sections\"],\n",
    "        \"topic\": state[\"messages\"][-1].content\n",
    "    })\n",
    "    return {\"outline\": outline}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 생성 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def generate_image(prompt):\n",
    "    \"\"\"Generate an image using DALL-E based on the given prompt\"\"\"\n",
    "    response = client.images.generate(\n",
    "        model=\"dall-e-3\",\n",
    "        prompt = prompt,\n",
    "        size=\"1024x1024\",\n",
    "        quality=\"standard\",\n",
    "        n=1\n",
    "    )\n",
    "    return response.data[0].url\n",
    "\n",
    "def image_generator(state: State):\n",
    "    prompt_template = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Base on the follwing section cotent, create a prompt for generationg an infographic that represents this section.\n",
    "\n",
    "        Section content:\n",
    "        {section_content}\n",
    "\n",
    "        Image generation prompt(under 1000 characters):\n",
    "        \"\"\",\n",
    "        input_variables=[\"section_content\"],\n",
    "    )\n",
    "    image_prompt = llm.invoke(prompt_template.format(section_content=state[\"section_content\"]))\n",
    "    image_url = generate_image(image_prompt.content)\n",
    "\n",
    "    current_section_number = state['current_section']\n",
    "    current_section = {\n",
    "        \"title\": state['outline'][f\"section{current_section_number}\"],\n",
    "        \"content\": state['section_content'],\n",
    "        \"image_url\": image_url,\n",
    "        \"image_prompt\": image_prompt.content if isinstance(image_prompt, AIMessage) else image_prompt\n",
    "    }\n",
    "\n",
    "    updated_full_report = state.get(\"full_report\", []) + [current_section]\n",
    "    print(f\"Completed section {current_section_number} of {state['total_sections']}\")\n",
    "\n",
    "    return {\n",
    "        \"image_prompt\": current_section[\"image_prompt\"],\n",
    "        \"section_image\": image_url,\n",
    "        \"current_section\": current_section_number + 1,\n",
    "        \"full_report\": updated_full_report,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컨텐츠 작성 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def contents_writer(state: State):\n",
    "    if 'error' in state:\n",
    "        return {'messages': [AIMessage(content=f\"An error occured: {state['error']}\")]}\n",
    "    \n",
    "    if state['current_section'] > state['total_sections']:\n",
    "        return {'messages': [AIMessage(content=f\"Report Completed\")]}\n",
    "    \n",
    "    current_section_number = state['current_section']\n",
    "    current_section_key = f\"section{current_section_number}\"\n",
    "    current_topic = state['outline'][current_section_key]\n",
    "    search_results = web_search.invoke(current_topic)\n",
    "\n",
    "    previous_sections_content = []\n",
    "    if (current_section_number > 1) :\n",
    "        for i, section in enumerate(state['full_report']):\n",
    "            print(f\"section_content: {section['title']}\\n\")\n",
    "            \n",
    "            previous_sections_content.append(f\"\"\"\n",
    "            Section {i+1}:\n",
    "            {section['title']}\n",
    "            {section['content']}\n",
    "            \"\"\")\n",
    "    # State에는 죄종만 section_content만 담는다. 확인 필요\n",
    "    #for i in range(1, current_section_number):\n",
    "        #section_key = f\"section{i}\"\n",
    "        #if section_key in state['section_content']:\n",
    "            # print(f\"section_content: {state['section_content'][section_key]}\\n\")\n",
    "            # previous_sections_content.append(f\"\"\"\n",
    "            # Section {i}:\n",
    "            # {state['outline'][section_key]}\n",
    "            # {state['section_content'][section_key]}\n",
    "            # \"\"\")\n",
    "    previous_sections = \"\\n\\n\".join(previous_sections_content)\n",
    "\n",
    "    section_prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        Write a detailed section for the topic: {topic}.\n",
    "\n",
    "        Use the following search results from information: {search_results}\n",
    "\n",
    "        Previous sections:\n",
    "        {previous_sections}\n",
    "        Write only the content for this section,\n",
    "        do not include any image prompts or suggestions.\n",
    "        Detailed statistics or information is needed, \n",
    "        so you should include collected information from search results.\n",
    "        \"\"\",\n",
    "        input_variables=['topic', 'search_results', 'previous_sections'],\n",
    "    )\n",
    "\n",
    "    section_content = llm.invoke(section_prompt.format(\n",
    "        topic = current_topic,\n",
    "        search_results = search_results,\n",
    "        previous_sections = previous_sections\n",
    "    ))\n",
    "\n",
    "    return {\n",
    "        'section_content': section_content.content,\n",
    "        'current_section': state['current_section'],\n",
    "    }\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "워드 생성 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "def report_generator(state: State):\n",
    "    doc = Document()\n",
    "    doc.add_heading(f\"Report: {state['messages'][0].content}\", 0)\n",
    "\n",
    "    for section in state['full_report']:\n",
    "        doc.add_heading(section['title'], level=1)\n",
    "        doc.add_paragraph(section['content'])\n",
    "\n",
    "        if section['image_url'] != \"Image generation failed\":\n",
    "            try:\n",
    "                response = requests.get(section['image_url'])\n",
    "                image = BytesIO(response.content)\n",
    "                doc.add_picture(image, width=Inches(6))\n",
    "                doc.add_paragraph(f\"Image prompt: {section['image_prompt']}\")\n",
    "            except Exception as e:\n",
    "                doc.add_paragraph(f\"Failed to add an image: {str(e)}\")\n",
    "\n",
    "        doc.add_page_break()\n",
    "    \n",
    "    filename = f\"report {state['messages'][0].content}.docx\".replace(\" \", \" \")\n",
    "    doc.save(filename)\n",
    "\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=f\"Report is finalized and saved as {filename}\")],\n",
    "        \"report_file\": filename\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"outline_generator\", outline_generator)\n",
    "graph_builder.add_node(\"contents_writer\", contents_writer)\n",
    "graph_builder.add_node(\"image_generator\", image_generator)\n",
    "graph_builder.add_node(\"report_generator\", report_generator)\n",
    "\n",
    "graph_builder.add_edge(START, \"outline_generator\")\n",
    "graph_builder.add_edge(\"outline_generator\", \"contents_writer\")\n",
    "graph_builder.add_edge(\"contents_writer\", \"image_generator\")\n",
    "graph_builder.add_edge(\"report_generator\", END)\n",
    "\n",
    "def should_continue_writing(state: State):\n",
    "    if state['current_section'] <= state['total_sections']:\n",
    "        return \"write_section\"\n",
    "    else:\n",
    "        return \"finalize_report\"\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"image_generator\",\n",
    "    should_continue_writing,\n",
    "    {\n",
    "        \"write_section\": \"contents_writer\",\n",
    "        \"finalize_report\": \"report_generator\",\n",
    "    },\n",
    ")\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import display_graph\n",
    "display_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 작업 디렉토리: /home/user/langchain-kr/98-Agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "print(f\"현재 작업 디렉토리: {current_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed section 1 of 5\n",
      "section_content: Introduction to GPTs in the KSD\n",
      "\n",
      "Completed section 2 of 5\n",
      "section_content: Introduction to GPTs in the KSD\n",
      "\n",
      "section_content: Current Landscape of Securities Depositories in Korea\n",
      "\n",
      "Completed section 3 of 5\n",
      "section_content: Introduction to GPTs in the KSD\n",
      "\n",
      "section_content: Current Landscape of Securities Depositories in Korea\n",
      "\n",
      "section_content: Benefits of Implementing GPTs at KSD\n",
      "\n",
      "Completed section 4 of 5\n",
      "section_content: Introduction to GPTs in the KSD\n",
      "\n",
      "section_content: Current Landscape of Securities Depositories in Korea\n",
      "\n",
      "section_content: Benefits of Implementing GPTs at KSD\n",
      "\n",
      "section_content: Challenges and Considerations for GPTs Adoption\n",
      "\n",
      "Completed section 5 of 5\n",
      "\n",
      "===보고서 생성 완료===\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "topic = input(\"보고서 주제를 입력하세요\")\n",
    "total_sections = int(input(\"생성할 섹션의 수를 입력하세요: \"))\n",
    "\n",
    "initial_state = {\n",
    "    \"messages\": [HumanMessage(content=topic)],\n",
    "    \"total_sections\": total_sections,\n",
    "    \"current_section\": 1,\n",
    "}\n",
    "\n",
    "for chunk in graph.stream(initial_state, stream_mode=\"update\"):\n",
    "    print(chunk)\n",
    "\n",
    "print(\"\\n===보고서 생성 완료===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KSD(Korea Securities Depositories)에서의 GPTs 도입 전략"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
